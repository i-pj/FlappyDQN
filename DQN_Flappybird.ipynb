{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flappy-bird-gymnasium\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bughhLxvfXSx",
        "outputId": "c4a05895-39d8-45f9-fdee-43c7f05375e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flappy-bird-gymnasium\n",
            "  Downloading flappy_bird_gymnasium-0.4.0-py3-none-any.whl (37.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium (from flappy-bird-gymnasium)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from flappy-bird-gymnasium) (1.25.2)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from flappy-bird-gymnasium) (2.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from flappy-bird-gymnasium) (3.7.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->flappy-bird-gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->flappy-bird-gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium->flappy-bird-gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flappy-bird-gymnasium) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flappy-bird-gymnasium) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flappy-bird-gymnasium) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flappy-bird-gymnasium) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flappy-bird-gymnasium) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flappy-bird-gymnasium) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flappy-bird-gymnasium) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flappy-bird-gymnasium) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->flappy-bird-gymnasium) (1.16.0)\n",
            "Installing collected packages: farama-notifications, gymnasium, flappy-bird-gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 flappy-bird-gymnasium-0.4.0 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w6Ts7JMgedJj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import random\n",
        "import flappy_bird_gymnasium\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = 'FlappyBird-v0'\n",
        "env = gym.make(env_id, render_mode=\"human\", use_lidar=True)\n",
        "eval_env = gym.make(env_id, render_mode=\"human\", use_lidar=True)\n",
        "\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n\n",
        "\n",
        "hidden_dim = 256\n",
        "batch_size = 64\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "n_episodes = 10000\n",
        "max_t = 1000\n",
        "print_every = 100"
      ],
      "metadata": {
        "id": "vT0JmLH_epxb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Model*\n"
      ],
      "metadata": {
        "id": "czy3JiOPe0mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((np.array(state), action, reward, np.array(next_state), done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        samples = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*samples)\n",
        "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
        "        actions = torch.tensor(actions)\n",
        "        rewards = torch.tensor(rewards)\n",
        "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
        "        dones = torch.tensor(dones)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "I_qNkb_zey3E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = DQN(s_size, a_size, hidden_dim).to(device)\n",
        "target_policy = DQN(s_size, a_size, hidden_dim).to(device)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
        "replay_buffer = ReplayBuffer(50000)\n",
        "\n",
        "scores = []\n",
        "for i_episode in range(1, n_episodes+1):\n",
        "    state = env.reset()[0]\n",
        "    score = 0\n",
        "    for t in range(max_t):\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).to(device)\n",
        "        action = torch.argmax(policy(state_tensor))\n",
        "        next_state, reward, done, info, _ = env.step(action.item())\n",
        "        score += reward\n",
        "        replay_buffer.add(state, action.item(), reward, next_state, done)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    scores.append(score)\n",
        "\n",
        "    if i_episode % 10 == 0:\n",
        "        target_policy.load_state_dict(policy.state_dict())\n",
        "\n",
        "    if len(replay_buffer.buffer) > batch_size:\n",
        "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "        states, actions, rewards, next_states, dones = states.to(device), actions.to(device), rewards.to(device), next_states.to(device), dones.to(device)\n",
        "        q_values = policy(states)\n",
        "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        next_q_values = target_policy(next_states)\n",
        "        next_q_values, _ = next_q_values.max(1)\n",
        "        dones = dones.float()\n",
        "        targets = rewards + gamma * next_q_values * (1 - dones)\n",
        "        loss = (q_values - targets).pow(2).mean()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if i_episode % print_every == 0:\n",
        "        print('Episode'+ str(i_episode) + '\\tAverage Score:'+ str(round(np.mean(scores[-print_every:]), 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIZQsVNQe7H1",
        "outputId": "1cfc199f-1a4b-4ee7-efe4-e0352432f9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode100\tAverage Score:-1.76\n",
            "Episode200\tAverage Score:-1.49\n",
            "Episode300\tAverage Score:-0.91\n",
            "Episode400\tAverage Score:-1.21\n",
            "Episode500\tAverage Score:-2.07\n",
            "Episode600\tAverage Score:-1.91\n",
            "Episode700\tAverage Score:-1.4\n",
            "Episode800\tAverage Score:-0.94\n",
            "Episode900\tAverage Score:-1.45\n",
            "Episode1000\tAverage Score:-1.79\n",
            "Episode1100\tAverage Score:1.67\n",
            "Episode1200\tAverage Score:1.63\n",
            "Episode1300\tAverage Score:0.26\n",
            "Episode1400\tAverage Score:-0.26\n",
            "Episode1500\tAverage Score:-0.41\n",
            "Episode1600\tAverage Score:0.73\n",
            "Episode1700\tAverage Score:-1.34\n",
            "Episode1800\tAverage Score:-0.05\n",
            "Episode1900\tAverage Score:0.09\n",
            "Episode2000\tAverage Score:-0.1\n",
            "Episode2100\tAverage Score:-1.39\n",
            "Episode2200\tAverage Score:-0.95\n",
            "Episode2300\tAverage Score:-0.32\n",
            "Episode2400\tAverage Score:2.09\n",
            "Episode2500\tAverage Score:-0.02\n",
            "Episode2600\tAverage Score:1.65\n",
            "Episode2700\tAverage Score:0.33\n",
            "Episode2800\tAverage Score:0.85\n",
            "Episode2900\tAverage Score:0.96\n",
            "Episode3000\tAverage Score:-1.06\n",
            "Episode3100\tAverage Score:-1.65\n",
            "Episode3200\tAverage Score:0.13\n",
            "Episode3300\tAverage Score:2.38\n",
            "Episode3400\tAverage Score:1.73\n",
            "Episode3500\tAverage Score:-0.13\n",
            "Episode3600\tAverage Score:-0.55\n",
            "Episode3700\tAverage Score:-0.51\n",
            "Episode3800\tAverage Score:-3.09\n",
            "Episode3900\tAverage Score:0.95\n",
            "Episode4000\tAverage Score:1.62\n",
            "Episode4100\tAverage Score:4.95\n",
            "Episode4200\tAverage Score:1.64\n",
            "Episode4300\tAverage Score:0.65\n",
            "Episode4400\tAverage Score:-0.58\n",
            "Episode4500\tAverage Score:0.33\n",
            "Episode4600\tAverage Score:0.82\n",
            "Episode4700\tAverage Score:0.67\n",
            "Episode4800\tAverage Score:2.42\n",
            "Episode4900\tAverage Score:1.52\n",
            "Episode5000\tAverage Score:1.11\n",
            "Episode5100\tAverage Score:0.03\n",
            "Episode5200\tAverage Score:-0.06\n",
            "Episode5300\tAverage Score:0.45\n",
            "Episode5400\tAverage Score:-0.31\n",
            "Episode5500\tAverage Score:0.07\n",
            "Episode5600\tAverage Score:0.92\n",
            "Episode5700\tAverage Score:0.06\n",
            "Episode5800\tAverage Score:0.22\n",
            "Episode5900\tAverage Score:0.45\n",
            "Episode6000\tAverage Score:0.07\n",
            "Episode6100\tAverage Score:1.84\n",
            "Episode6200\tAverage Score:2.71\n",
            "Episode6300\tAverage Score:1.77\n",
            "Episode6400\tAverage Score:1.06\n",
            "Episode6500\tAverage Score:1.51\n",
            "Episode6600\tAverage Score:1.06\n",
            "Episode6700\tAverage Score:1.45\n",
            "Episode6800\tAverage Score:2.22\n",
            "Episode6900\tAverage Score:1.32\n",
            "Episode7000\tAverage Score:1.71\n",
            "Episode7100\tAverage Score:1.55\n",
            "Episode7200\tAverage Score:0.67\n",
            "Episode7300\tAverage Score:1.66\n",
            "Episode7400\tAverage Score:1.97\n",
            "Episode7500\tAverage Score:1.68\n",
            "Episode7600\tAverage Score:1.55\n",
            "Episode7700\tAverage Score:1.37\n",
            "Episode7800\tAverage Score:2.24\n",
            "Episode7900\tAverage Score:1.67\n",
            "Episode8000\tAverage Score:2.6\n",
            "Episode8100\tAverage Score:3.06\n",
            "Episode8200\tAverage Score:1.47\n",
            "Episode8300\tAverage Score:2.71\n",
            "Episode8400\tAverage Score:2.08\n",
            "Episode8500\tAverage Score:2.59\n",
            "Episode8600\tAverage Score:2.57\n",
            "Episode8700\tAverage Score:2.48\n",
            "Episode8800\tAverage Score:2.22\n",
            "Episode8900\tAverage Score:0.57\n",
            "Episode9000\tAverage Score:-0.33\n",
            "Episode9100\tAverage Score:0.27\n",
            "Episode9200\tAverage Score:-0.35\n",
            "Episode9300\tAverage Score:-0.15\n",
            "Episode9400\tAverage Score:0.22\n",
            "Episode9500\tAverage Score:-1.59\n",
            "Episode9600\tAverage Score:-0.24\n",
            "Episode9700\tAverage Score:0.93\n",
            "Episode9800\tAverage Score:0.95\n",
            "Episode9900\tAverage Score:2.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(policy.state_dict(), 'dqn_policy.pth')"
      ],
      "metadata": {
        "id": "k0VrnwHAfLjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7G7CHItp-d1d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}